<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>KD Pytorch</title>
      <link href="/2021/06/11/kd-pytorch/"/>
      <url>/2021/06/11/kd-pytorch/</url>
      
        <content type="html"><![CDATA[<p>KD Pytorch</p><a id="more"></a><h4 id="KD-Pytorch"><a href="#KD-Pytorch" class="headerlink" title="KD Pytorch"></a>KD Pytorch</h4><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> CrossEntropyLoss<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset<span class="token punctuation">,</span>DataLoader<span class="token punctuation">,</span>SequentialSampler<span class="token keyword">class</span> <span class="token class-name">model</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>input_dim<span class="token punctuation">,</span>hidden_dim<span class="token punctuation">,</span>output_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>model<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span>hidden_dim<span class="token punctuation">,</span>output_dim<span class="token punctuation">,</span>batch_first <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span>output_dim<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>        layer1_output<span class="token punctuation">,</span>layer1_hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>        layer2_output <span class="token operator">=</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span>layer1_output<span class="token punctuation">)</span>        layer2_output <span class="token operator">=</span> layer2_output<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#取出一个batch中每个句子最后一个单词的输出向量即该句子的语义向量！！！！！！！!！</span>        <span class="token keyword">return</span> layer2_output<span class="token comment" spellcheck="true">#建立小模型</span>model_student <span class="token operator">=</span> model<span class="token punctuation">(</span>input_dim <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>hidden_dim <span class="token operator">=</span> <span class="token number">8</span><span class="token punctuation">,</span>output_dim <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#建立大模型（此处仍然使用LSTM代替，可以使用训练好的BERT等复杂模型）</span>model_teacher <span class="token operator">=</span> model<span class="token punctuation">(</span>input_dim <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>hidden_dim <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">,</span>output_dim <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#设置输入数据，此处只使用随机生成的数据代替</span>inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>true_label <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#生成dataset</span>dataset <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span>true_label<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#生成dataloader</span>sampler <span class="token operator">=</span> SequentialSampler<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset <span class="token operator">=</span> dataset<span class="token punctuation">,</span>sampler <span class="token operator">=</span> sampler<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span>loss_fun <span class="token operator">=</span> CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>criterion  <span class="token operator">=</span> nn<span class="token punctuation">.</span>KLDivLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#KL散度</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model_student<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">,</span>momentum <span class="token operator">=</span> <span class="token number">0.9</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#优化器，优化器中只传入了学生模型的参数，因此此处只对学生模型进行参数更新，正好实现了教师模型参数不更新的目的</span><span class="token keyword">for</span> step<span class="token punctuation">,</span>batch <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span>    inputs <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    labels <span class="token operator">=</span> batch<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true">#分别使用学生模型和教师模型对输入数据进行计算</span>    output_student <span class="token operator">=</span> model_student<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>    output_teacher <span class="token operator">=</span> model_teacher<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#计算学生模型预测结果和教师模型预测结果之间的KL散度</span>    loss_soft <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output_student<span class="token punctuation">,</span>output_teacher<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#计算学生模型和真实标签之间的交叉熵损失函数值</span>    loss_hard <span class="token operator">=</span> loss_fun<span class="token punctuation">(</span>output_student<span class="token punctuation">,</span>labels<span class="token punctuation">)</span>    loss <span class="token operator">=</span> <span class="token number">0.9</span><span class="token operator">*</span>loss_soft <span class="token operator">+</span> <span class="token number">0.1</span><span class="token operator">*</span>loss_hard    <span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Rethink Overfitting：Overfitting 的传统和现代观点有什么不同呢？</title>
      <link href="/2021/06/05/rethink-overfitting-overfitting-de-chuan-tong-he-xian-dai-guan-dian-you-shi-me-bu-tong-ni/"/>
      <url>/2021/06/05/rethink-overfitting-overfitting-de-chuan-tong-he-xian-dai-guan-dian-you-shi-me-bu-tong-ni/</url>
      
        <content type="html"><![CDATA[<p>做了一道清叉大佬的测试题</p><a id="more"></a><h4 id="Rethink-Overfitting：Overfitting-的传统和现代观点有什么不同呢？"><a href="#Rethink-Overfitting：Overfitting-的传统和现代观点有什么不同呢？" class="headerlink" title="Rethink Overfitting：Overfitting 的传统和现代观点有什么不同呢？"></a>Rethink Overfitting：Overfitting 的传统和现代观点有什么不同呢？</h4><h5 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a><strong>介绍</strong></h5><p>过度拟合作为机器学习的一个传统且重要的主题，已经得到了大量坚实的基础理论和经验证据的充分研究。然而，随着近年来深度学习 (DL) 的突破正在迅速改变科学和社会，在Machine Learning中观察到许多似乎与“经典”过拟合理论相矛盾或无法彻底解释的现象。</p><h4 id="传统观点中的过拟合"><a href="#传统观点中的过拟合" class="headerlink" title="传统观点中的过拟合"></a>传统观点中的过拟合</h4><p><strong>过拟合</strong></p><p>过拟合的经典定义为给定一个假设空间H，一个假设h属于H，如果存在其他的假设h’属于H,使得在训练样例上h的错误率比h’小，但在整个实例分布上h’比h的错误率小，那么就说假设h过度拟合训练数据。这与经验风险最小化 (ERM) 框架下的表达是十分一致的，我们定义：</p><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210605205032766.png" alt="image-20210605205032766" style="zoom:40%;"><p>在 ERM 框架下，当我们模型的经验（训练）风险与真实（测试）风险相比相对较小时，就会发生过度拟合。在等式中，<em>h</em>指的是我们的预测模型，<em>l</em>是一些损失函数。不管定义的形式如何，过拟合的传统定义只关注模型在潜在“真实”数据分布上的性能。</p><p><strong>偏差-方差权衡</strong></p><p>偏差-方差权衡理论经常与过拟合一起出现，为如何检测和防止过拟合提供理论指导，可以总结为经典的 U 形风险曲线，传统的理解是，我们需要找到欠拟合和过拟合之间的“甜蜜点”。当达到合适的平衡时，预测器<em>h</em>在训练数据上的性能可以被很好的泛化到真实的数据分布。</p><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210605205429800.png" alt="image-20210605205429800" style="zoom:33%;"><p>经典过拟合理论提供了以下见解：</p><ul><li>在测试数据上表现不佳而在训练数据上表现良好则表明过拟合。</li><li>U 形偏差-方差权衡曲线表明，在最佳点之外，泛化性能随着模型变得更加复杂而降低。</li><li>使用高度复杂的模型而不考虑过度拟合是有风险的，因而我们需要使用显式的正则化。</li></ul><h4 id="Overfitting的现代观点"><a href="#Overfitting的现代观点" class="headerlink" title="Overfitting的现代观点"></a>Overfitting的现代观点</h4><p><strong>双下降曲线</strong> </p><p>传统的机器学习使用以下 U 形风险曲线来衡量偏差-方差权衡并量化模型的泛化程度。当模型过于复杂并且在测试集上的准确度比在训练集上差得多时，就会发生“过度拟合”。这种理解使得我们使用正则化的方法来限制模型复杂性。我们不希望看到零训练误差，因为这在传统的观点中相当于过拟合。随着模型变大（添加更多参数），训练误差减小到接近于零，但是一旦模型复杂度增长到超过“欠拟合”和“过拟合”之间的阈值，测试误差（泛化误差）开始增加。</p><p>然而这不适用于深度学习模型，<a href="https://arxiv.org/abs/1812.11118" target="_blank" rel="noopener">Belkin et al. (2018)</a>调和了传统的偏差-方差权衡，并为深度神经网络提出了一种新的双 U 形风险曲线。一旦网络参数的数量足够多，风险曲线就会进入另一个状态。</p><p>与广为接受的“U 形”曲线相反，从现代深度学习模型可以观察到双下降曲线，这条“双下降”曲线结合了 U 形风险曲线，由“插值阈值”分隔。这里的“插值”是指模型经过训练以精确拟合数据，因此插值阈值右侧的模型具有零训练风险。 </p><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210605211323014.png" alt="image-20210605211323014" style="zoom:50%;"><p>现代过拟合观点提供了以下见解：</p><ul><li>过拟合可以用许多其他形式表示。这取决于需要将模型用于什么目的。</li><li>在插值点之外，我们可以在适当的实验条件下观察到另一个测试误差的下降，非常复杂的模型不一定会导致过度拟合。</li><li>对于 DNN，显式正则化似乎更像是一个有助于提高泛化的调整参数，但它的缺失并不一定意味着泛化错误很差。</li></ul><p>在深度学习时代，我们需要重新思考过拟合的定义。人们观察到高度参数化的神经网络在测试集上泛化良好，并且实现了零训练损失，Belkin提出的双下降曲线统一了以前和现代的过拟合机制，并显示了复杂模型在模型足够“复杂”时能够很好地泛化。总之，没有单一因素决定模型的泛化能力。模型架构、显式和隐式正则化以及数据集设计都发挥了作用。</p><p>Reference：[Belkin et al. (2018)] [(<a href="https://arxiv.org/abs/1812.11118)]" target="_blank" rel="noopener">https://arxiv.org/abs/1812.11118)]</a> <strong>Reconciling modern machine learning practice and the bias-variance trade-off</strong></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>BERT-ATTACK-- Adversarial Attack Against BERT Using BERT</title>
      <link href="/2021/05/31/bert-attack-adversarial-attack-against-bert-using-bert/"/>
      <url>/2021/05/31/bert-attack-adversarial-attack-against-bert-using-bert/</url>
      
        <content type="html"><![CDATA[<p>最近读了一点论文</p><a id="more"></a><h4 id="BERT-ATTACK-Adversarial-Attack-Against-BERT-Using-BERT"><a href="#BERT-ATTACK-Adversarial-Attack-Against-BERT-Using-BERT" class="headerlink" title="BERT-ATTACK: Adversarial Attack Against BERT Using BERT"></a>BERT-ATTACK: Adversarial Attack Against BERT Using BERT</h4><p>Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu∗Shanghai Key Laboratory of Intelligent Information Processing, Fudan University EMNLP2020</p><h5 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h5><p>针对离散数据很难使用基于梯度的方法生成对抗样本，所以对于离散的数据（例如文本）的对抗攻击比连续数据（例如图像）更具挑战性。当前比较成功的文本攻击方法通常在字符或单词上采用启发式替换策略，然而这依然很难在如此庞大的可替换词选项中去找到一个最优的解决办法并同时保持语义一致性和语言流畅性。</p><p>在本文中，作者提出了一个方法叫<strong>BERT-Attack</strong>，这是一种高质量且有效生成对抗性样本的方法，可以使用以BERT为例的MLM预训练语言模型（masked language models）来生成对抗性样本。作者使用BERT对抗其微调模型和其他预训练模型，以误导目标模型，使其预测错误。作者的方法在成功率和扰动率方面均优于目前最优模型SOTA，并且生成的对抗性样本很流利，并且在语义一致。而且作者的方法计算成本低，可以大规模生成。</p><ul><li><h5 id="What-is-the-problem-the-paper-is-trying-to-address-论文试图解决什么问题？"><a href="#What-is-the-problem-the-paper-is-trying-to-address-论文试图解决什么问题？" class="headerlink" title="What is the problem the paper is trying to address? 论文试图解决什么问题？"></a>What is the problem the paper is trying to address? 论文试图解决什么问题？</h5><p>虽然在计算机视觉领域，攻击策略及其防御对策都得到了很好的探索，但由于语言的离散性，对文本的攻击仍然具有挑战性。本文针对离散型的文本的对抗攻击进行了研究，为文本生成对抗样本需要具备这样的品质：(1)人的判断是不可察觉的，而神经模型是不可感知的  (2)流利的语法和语义与原始输入一致。</p><p>而且业内目前的方法还存在一些问题，当前比较成功的文本攻击方法通常在字符或单词上采用启发式替换策略，然而这依然很难在如此庞大的可替换词选项中去找到一个最优的解决办法并同时保持语义一致性和语言流畅性。作者希望在这一方面进行探索最终的目标是得到高准确率低扰动率同时保持文本的流利，并且在语义上一致。</p></li><li><h5 id="What-is-the-key-of-the-proposed-solution-in-the-paper-论文中提到的解决方案之关键是什么？"><a href="#What-is-the-key-of-the-proposed-solution-in-the-paper-论文中提到的解决方案之关键是什么？" class="headerlink" title="What is the key of the proposed solution in the paper? 论文中提到的解决方案之关键是什么？"></a>What is the key of the proposed solution in the paper? 论文中提到的解决方案之关键是什么？</h5><p>早先的工作有：Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment AAAI2020 作者提出了一种对抗样本生成算法<strong>TEXTFOOLER</strong>。论文中，作者使用这种方法，对文本分类与文本蕴含两种任务做了测试，成功的攻击了这两种任务的相关模型，包括：BERT,CNN,LSTM,ESIM等等。</p><p>除此之外，以前的方法主要根据特定规则制作对抗样本(Li et al., 2018; Gao et al., 2018; Yang et al., 2018; Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2019; Zang et al., 2020).因此，这些方法难以同时保证生成的对抗样本的流畅性和语义保留。 另外，这些方法步骤相当复杂。 他们使用多种语言约束，如 NER 标记或 POS 标记。 引入上下文语言模型作为自动扰动生成器可以使这些规则的设计更加容易。</p><p>BERT (Devlin et al., 2018),将 NLP 任务的性能提升到一个新的水平。 一方面，经过微调的 BERT 在下游任务上的强大能力使得对抗性攻击更具挑战性（Jin et al., 2019）。 另一方面，BERT 是在超大规模无监督数据上预训练的MLM模型，学习了通用语言知识。因此，BERT 有可能为输入文本生成更流畅和语义一致的替换。 自然地，BERT 的这两个特性促使他们探索用另一个 BERT 作为攻击者来攻击一个微调的 BERT 的可能性。</p><p>论文提出了<strong>BERTAttack</strong>，使用 BERT 作为语言模型。BERT-Attack 的核心算法很简单，包括两个阶段：在目标模型的一个给定输入序列中查找易受攻击的词； 然后以保留语义的方式应用 BERT 来生成易受攻击的词的替代品。</p><p>凭借 BERT 的能力，可以考虑周围的上下文来生成扰动。 因此，扰动是流畅和合理的。 作者使用masked language model作为扰动生成器，并找到使错误预测风险最大化的扰动。 与之前需要传统单向语言模型作为约束的攻击策略不同，我们只需将语言模型作为扰动生成器进行一次推断，而不是在反复试验的过程中反复使用语言模型对生成的对抗样本进行评分。</p><p><strong>相关工作</strong>：当前成功的文本攻击通常采用启发式规则来修改单词的字符 (Jin et al.,2019),并用同义词替换单词(Ren et al., 2019)。Li et al. (2018); Gao et al. (2018)应用基于词嵌入的扰动，例如 Glove(Pennington et al., 2014)但这在语义和语法上没有严格协调。Alzantot et al. (2018)采用语言模型对通过在词嵌入空间中搜索接近意义的词而产生的扰动进行评分 ，使用试错过程来寻找可能的扰动，但产生的扰动仍然存在。不是上下文感知的，并且严重依赖于词嵌入的余弦相似度测量。Glove嵌入不保证具有余弦相似距离的相似向量空间，因此扰动在语义上不太一致。还有很多的相关工作在此就不一一赘述了。</p></li><li><h5 id="How-are-the-experiments-designed-论文中的实验是如何设计的？"><a href="#How-are-the-experiments-designed-论文中的实验是如何设计的？" class="headerlink" title="How are the experiments designed? 论文中的实验是如何设计的？"></a>How are the experiments designed? 论文中的实验是如何设计的？</h5><p><strong>原理：</strong></p><p>BertAttack包括两个步骤：（1）找到目标模型的易受攻击的词，然后（2）用语义相似且语法正确的词替换它们，直到成功攻击。最脆弱的词是帮助目标模型做出判断的关键词。 对这些词的扰动对制作对抗样本最有利。 在找到我们要替换的单词后，我们使用MLM根据MLM的 top-K 预测生成扰动。</p><p><strong>查找易受攻击的词</strong></p><p>$S=[w_0,w_1……w_n]$代表输入的句子$o_y（S）$表示目标模型对正确标签 y 的 logit ，重要性$I_{w_i}=o_y（S）-o_y（S_{\w_i}）$这里$S_{\w_i}=[w_0,w_1…[MASK]…w_n]$输出作者给句子中的每一个词一个评分，得分与易受攻击程度呈正比，该评分按照去掉该词的句子在判别器上的输出结果的扰动程度给出。作者使用目标模型（微调的BERT或其他神经模型）的logit输出作为判别器。易受攻击词定义为序列中对最终输出logit有重要影响的单词。令表示输入语句，表示目标模型输出的正确标签y的logit。</p><p><strong>通过BERT替换单词</strong></p><p><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210531171439651.png" alt="image-20210531171439651" style="zoom:25%;"><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210531212632572.png" alt="image-20210531212632572" style="zoom:25%;"></p><p>找到易受攻击的单词后，将列表L中的单词一一替换，以寻找可能误导目标模型的干扰。以前的替换方法包括同义词词典，POS检查器，语义相似性检查器等。但是因为替换的时候只有词表，不考虑上下文，因此需要用传统语言模型给替换单词的句子打分。由于换一个词就得评价一次，时间成本比较高。</p><p>作者利用BERT进行单词替换，可确保所生成的句子相对流利且语法正确，还保留了大多数语义信息。此外，掩码语言模型的预测是上下文感知的，因此可以动态搜索扰动，而不是简单的同义词替换。而且针对一个词而言，仅通过一个前向即可产生候选文本，无需再用语言模型来对句子评分，提升了效率。</p><p><strong>词替换策略</strong></p><p>给定要替换的选定单词 w，应用 BERT 来预测可能与 w 相似但可能误导目标模型的单词。论文不遵循MLM设置，而是不屏蔽所选单词 w 并使用原始序列作为输入，这可以生成更多语义一致的替代词。例如，给定一个序列“I like the cat”，如果我们屏蔽单词 cat，则屏蔽语言模型将很难预测原始单词 cat，因为如果序列是“I like dog“同样流利。此外，如果我们屏蔽给定的词 w，对于每次迭代，我们将不得不重新运行MLM预测过程，这是成本高昂的。</p><p>让 M 表示 BERT 模型，我们将标记化的序列 H 输入到 BERT M 中以获得输出预测 P = M(H)。 不使用 argmax 预测，而是在每个位置采用最可能的 K 个预测。我们迭代按单词重要性排序过程排序的单词以查找扰动。 BERT 模型使用 BPE 编码来构建词汇表。 虽然大多数词仍然是单个词，但稀有词被标记为子词。 因此，我们分别对待单个词和子词来生成替代词。实现算法如代码部分所示。</p><p><strong>实验设计</strong>：</p><p>作者应用这个方法以文本分类和自然语言推理的形式来攻击不同类型的NLP任务。作者从给定任务的测试集中随机选取1k个测试样本来评估BertAttack。首先是，文本分类任务，作者使用不同类型的文本分类任务来研究作者的方法的有效性。然后是自然语言推理，给出一个前提和一个假设，目标是预测假设是相吻合的、不相关的还是与前提相矛盾的。</p><p>为了衡量所生成样本的质量，作者设计了几种评估指标：成功率（success rate）：攻击样本的判别器准确率。扰动百分比（perturbed percentage）更改文本的占比。每个样本的查询数量（query number per sample）一个样本生成对抗样本的需要访问判别器的次数。语义相似度（semantic similarity）使用通用句子编码器（Universal Sentence Encoder）评价的句子相似度。</p></li><li><h5 id="What-datasets-are-built-used-for-the-quantitative-evaluation-Is-the-code-open-sourced-客观描述：用于定量评估的数据集是什么？代码有没有开源？"><a href="#What-datasets-are-built-used-for-the-quantitative-evaluation-Is-the-code-open-sourced-客观描述：用于定量评估的数据集是什么？代码有没有开源？" class="headerlink" title="What datasets are built/used for the quantitative evaluation? Is the code open sourced? 客观描述：用于定量评估的数据集是什么？代码有没有开源？"></a>What datasets are built/used for the quantitative evaluation? Is the code open sourced? 客观描述：用于定量评估的数据集是什么？代码有没有开源？</h5><p><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210608000638336.png" alt="image-20210608000638336" style="zoom:40%;"><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210608000659376.png" alt="image-20210608000659376" style="zoom:40%;"></p><p>用于定量评估的数据集有Yelp 评论分类数据集，IMDB电影评论数据集，其中平均序列长度比Yelp数据集长。实验处理数据集Yelp和IMDB来构造一个极性分类任务。AG’s News新闻类型分类数据集，包含4类news:World、体育、商业和科学。FAKE假新闻分类数据集，检测Kaggle假新闻挑战中的新闻是否为假。</p><p>SNLI Stanford语言推理任务给出一个前提和一个假设，目标是预测假设是相吻合的、不相关的还是与前提相矛盾的。多体裁文本的MNLI语言推理数据集，涵盖转录语音、流行小说和政府报告（Williamset al.，2018），与SNLI数据集相比，该数据集更为复杂，包括与训练域匹配的评估数据和与训练域不匹配的评估数据</p><p>代码已开源：<a href="https://github.com/LinyangLee/BERT-Attack" target="_blank" rel="noopener">https://github.com/LinyangLee/BERT-Attack</a></p></li><li><h5 id="Is-the-scientific-hypothesis-well-supported-by-evidence-in-the-experiments-论文中的实验及结果有没有很好地支持需要验证的科学假设？"><a href="#Is-the-scientific-hypothesis-well-supported-by-evidence-in-the-experiments-论文中的实验及结果有没有很好地支持需要验证的科学假设？" class="headerlink" title="Is the scientific hypothesis well supported by evidence in the experiments?论文中的实验及结果有没有很好地支持需要验证的科学假设？"></a>Is the scientific hypothesis well supported by evidence in the experiments?论文中的实验及结果有没有很好地支持需要验证的科学假设？</h5><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210531214826824.png" alt="image-20210531214826824" style="zoom:40%;"><p>BERT攻击方法成功地使其下游的微调模型发生错误。在文本分类和自然语言推理任务中，微调的BERTs不能正确地对生成的对抗性样本进行分类，攻击后的平均准确率低于10%，说明大多数样本成功地使现有的分类模型发生错误。此外，BERT攻击成功地攻击了所有列出的任务，这些任务位于不同的领域，如新闻分类、评论分类、语言推理等。结果表明，该攻击方法对不同的任务具有较强的鲁棒性。</p><p>BertAttack的查询数和干扰百分比是非常少的，我们可以观察到，由于干扰百分比非常低，所以通常更容易处理评论分类任务。BERT攻击只能通过替换少量的单词来误导目标模型。由于平均序列长度相对较长，目标模型往往只通过序列中的几个词来判断，这不是人类预测的自然方式。因此，这些关键字的扰动将导致目标模型的正确预测，从而揭示其脆弱性。</p><p>为了进一步评估生成的对抗句样本，作者建立了人工评估来衡量生成样本在流畅性、语法和语义保留方面的质量。作者使用IMDBdataset和MNLI数据集，为每个任务选择100个原始样本和对抗样本作为人类判断。</p><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210531215228561.png" alt="image-20210531215228561" style="zoom:50%;"><p>对抗性样本的语义得分和语法得分与原始样本接近。MNLI任务是由人类根据前提构建的一个句子对预测任务，因此原始句子共享相当多的相同单词，对这些单词的扰动会使人类判断者很难正确预测，因此准确率低于简单的句子分类任务</p><p>BERT攻击方法也适用于攻击其它目标模型，不局限于其微调模型。对基于LSTM的模型的攻击是成功的，这表明BERT攻击在广泛的模型中是可行的。在BERT攻击下，ESIM模型在MNLI数据集中更为健壮。作者认为把两句话分开编码能获得更好的稳健性。在攻击BERT大模型时，其性能也很好，说明BERT攻击不仅针对其自身的精调下游模型，而且成功地攻击了不同的预训练模型。</p><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210531215544942.png" alt="image-20210531215544942" style="zoom:50%;"></li><li><h5 id="What-are-the-contributions-of-the-paper-这篇论文到底有什么贡献？What-could-be-done-next-下一步呢？有什么工作可以继续深入？"><a href="#What-are-the-contributions-of-the-paper-这篇论文到底有什么贡献？What-could-be-done-next-下一步呢？有什么工作可以继续深入？" class="headerlink" title="What are the contributions of the paper?这篇论文到底有什么贡献？What could be done next? 下一步呢？有什么工作可以继续深入？"></a>What are the contributions of the paper?这篇论文到底有什么贡献？What could be done next? 下一步呢？有什么工作可以继续深入？</h5><p>这篇文章提出了一种高质量和有效的方法Bert Attack，以BERT为例的MLM预训练语言模型来生成对抗性样本。作者使用BERT对抗其微调模型和其他预训练模型，以误导目标模型，使其预测错误。作者的方法在成功率和扰动百分比方面均优于最新的攻击策略，并且生成的对抗性样本较流利，并且在语义一致上有所改进。而且BertAttack方法计算成本低，可以大规模生成。这种攻击方法无疑对目前的预训练语言模型Bert提出了巨大的挑战，这让我们知道尽管具有高准确率的Bert模型还是有很多可改进的地方的，这些攻击方法的提出可以带动防御方法针对性的研究因为最终的目标是确保神经网络是高度健壮可靠的。</p><p>下一步的工作我认为可以分为两个方面第一是目前这个攻击方法还不是很完善，尽管该实验表明这个方法会在保证最小扰动的前提下进行攻击，然而通过代码实验可以发现MLM生成的候选词有时是与原文中替换的词词意相反以及无关的，这会造成语意的损失。因此，增强语言模型以产生更多语义相关的扰动可能是未来完善BERT攻击的一个可能的解决方案。除此之外，对于这个攻击方法进行相关防御方法的研究意义也是非常重大的。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Do Adversarially Robust ImageNetModels Transfer Better?</title>
      <link href="/2021/05/31/do-adversarially-robust-imagenetmodels-transfer-better/"/>
      <url>/2021/05/31/do-adversarially-robust-imagenetmodels-transfer-better/</url>
      
        <content type="html"><![CDATA[<p>最近读了一点论文</p><a id="more"></a><h4 id="Do-Adversarially-Robust-ImageNetModels-Transfer-Better"><a href="#Do-Adversarially-Robust-ImageNetModels-Transfer-Better" class="headerlink" title="Do Adversarially Robust ImageNetModels Transfer Better?"></a>Do Adversarially Robust ImageNetModels Transfer Better?</h4><p>Hadi Salman，Andrew Ilyas，Logan Engstrom，Ashish Kapoor，Aleksander Madry Microsoft Research，MIT  ICML2020</p><h5 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h5><p>在深度学习中，迁移学习是一种广泛使用的模式。在这种模式中，在标准数据集上预先训练的模型可以有效地适应下游任务。通常，更好的预训练模型产生更好的迁移结果，这表明初始准确性是迁移学习表现的一个关键方面。在这项工作中，我们确定了这样的另一个方面:当用于迁移学习时，虽然对抗鲁棒模型的准确度降低，但通常比标准训练的对应模型表现得更好。具体来说，我们关注的是具有对抗鲁棒性的ImageNet分类器，并展示了它们在下游分类任务上提高了准确性。进一步的分析揭示了迁移学习背景下鲁棒模型和标准模型的更多差异。我们的结果与最近提出的假设是一致的，即鲁棒性导致改进的特征表征。</p><ul><li><h5 id="What-is-the-problem-the-paper-is-trying-to-address-论文试图解决什么问题？"><a href="#What-is-the-problem-the-paper-is-trying-to-address-论文试图解决什么问题？" class="headerlink" title="What is the problem the paper is trying to address? 论文试图解决什么问题？"></a>What is the problem the paper is trying to address? 论文试图解决什么问题？</h5><p>本文工作的动机建立在提升迁移学习性能上，迁移学习是指利用来自一个（“源”）任务的信息来更好地解决另一个（“目标”）任务的任何机器学习算法。预先训练的具有更高精度的ImageNet模型可以产生更好的固定特征转移学习结果。计算机视觉中的典型迁移学习pipeline（也是我们工作的重点）始于在 ImageNet-1K 数据集上训练的模型 ，然后为目标任务细化这个模型。不过目前我们还不知道提高 ImageNet 准确率是否是提高性能的唯一途径。 毕竟，固定特征迁移的行为受模型学习表示的控制，而源数据集的准确性并没有完全描述这些表示。在这项工作中，作者把注意力转向另一个点对抗性鲁棒性，对抗鲁棒性是否有助于迁移学习？</p></li><li><h5 id="What-is-the-key-of-the-proposed-solution-in-the-paper-论文中提到的解决方案之关键是什么？"><a href="#What-is-the-key-of-the-proposed-solution-in-the-paper-论文中提到的解决方案之关键是什么？" class="headerlink" title="What is the key of the proposed solution in the paper? 论文中提到的解决方案之关键是什么？"></a>What is the key of the proposed solution in the paper? 论文中提到的解决方案之关键是什么？</h5><p>本文通过大量的实验对这一猜想进行了验证，对于每个迁移学习范式，作者报告了标准和稳健模型的十次随机试验的平均准确率。 同时使用网格搜索（使用一组不相交的随机子集）来找到最佳的超参数、架构和（对于稳健模型）鲁棒性水平 ε。 </p><p>在这项工作中，我们将注意力转向另一个先验：对抗性鲁棒性。对抗性鲁棒性是指模型对其输入的小（通常难以察觉）扰动的不变性。通常在训练时通过用稳健优化目标 [MMS+18] 替换标准经验风险最小化目标来诱导稳健性：</p><p>![image-20210609203215334](/Users/mac/Library/Application Support/typora-user-images/image-20210609203215334.png)</p><p>其中 ε 是一个超参数，用于控制生成的“对抗性鲁棒模型”（更简单地说，“鲁棒模型”）的不变性。简而言之，该目标要求模型将训练数据点的风险降至最低，同时在每个点周围的 (radius-ε) 邻域中保持局部稳定。</p><p>对抗性鲁棒性最初是在机器学习安全 [BCM+13; BR18; CW17] 作为一种提高模型对对抗性示例的弹性的方法 [GSS15; 彩信+18]。 然而，最近的一项工作本身研究了对抗性鲁棒模型，将 (1) 作为学习特征表示的先验 [EIS+19b; ISE+19； JBZ+19； ZZ19]。</p><p>从迁移学习的角度来看，“对抗性鲁棒性”的预期是不明确的。一方面，对抗样本的鲁棒性似乎与转移性能有些相关。事实上，对抗鲁棒模型的精确度明显低于标准模型[TSE+19；SZC+18；RXY+19；Nak19]，而迁移模型的初始准确率却会大大的影响模型的性能，这表明使用对抗性健壮的特征表示会损害传输性能。</p><p>另一方面，最近的工作发现鲁棒模型的特征表示比标准模型的特征表示有几个优点。例如，对抗性鲁棒表示通常具有更好的梯度 [TSE+19; STT+19； ZZ19; KCL19]，从而促进无正则化特征可视化 [EIS+19b]。鲁棒表示也是近似可逆的 [EIS+19b]，这意味着与标准模型不同 [MV15; [DB16]，可以从其鲁棒性表示中近似直接重建图像（请参见图1b）。Engstrom等人假设通过强制网络对人类也不变的信号保持不变，稳健的训练目标会导致特征表示与人类使用的更相似。这表明，从迁移学习的角度来看，对抗性稳健性可能是一种可取的先验知识。本文就是想要研究对抗鲁棒性对于迁移学习的影响，解决这两个相互冲突的假设，为了解决这两个相互冲突的假设，使用12个标准迁移学习数据集的试验台来评估标准和不利稳健ImageNet模型上的固定特征传输。</p><p><strong>相关工作</strong> </p><ul><li>最近，[ZSS+18] 提出了一种方法，当来自多个领域的标记数据可用时，可以提高迁移学习的效率。此外，研究了更好的 ImageNet 模型是否能更好地转移到其他数据集 [KSL19]。它显示了预训练 ImageNet 模型的传输精度（逻辑回归和微调设置）与这些模型在 ImageNet 上的 top-1 精度之间存在很强的相关性。最后，[KBZ+19] 使用大约 3 亿张噪声标记图像的海量数据探索了预训练，并展示了在 ImageNet 上针对多个任务进行预训练的迁移学习的改进。</li><li>有几项工作研究了修改源数据集如何影响传输精度。 [ARS+15; HAE16] 研究了类数量与每类图像数量对学习更好的固定图像特征的重要性，这些工作得出了相互矛盾的结论 [KSL19]。</li></ul></li><li><h5 id="How-are-the-experiments-designed-论文中的实验是如何设计的？"><a href="#How-are-the-experiments-designed-论文中的实验是如何设计的？" class="headerlink" title="How are the experiments designed? 论文中的实验是如何设计的？"></a>How are the experiments designed? 论文中的实验是如何设计的？</h5><p>为了解决这两个相互矛盾的假设，作者使用了 12 个标准迁移学习数据集（[KSL19] 和 Caltech-256 [GHP07] 中考虑的所有数据集）的测试台来评估标准和对抗性鲁棒 ImageNet 模型上的固定特征迁移，考虑了四种基于 ResNet 的架构（ResNet-{18,50}、WideResNet-50-x{2,4}），并针对每种架构训练具有不同鲁棒性级别 ε 的模型。</p><p><strong>模型结构</strong>  作者考虑了四种基于ResNet的体系结构（ResNet-{18,50}，WideResNet-50-x{2,4}），并训练具有不同鲁棒性水平的模型ε对于每种体系结构，作者比较了标准模型和具有相同体系结构的最佳鲁棒模型。</p><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210609204122633.png" alt="image-20210609204122633" style="zoom:35%;"><p><strong>训练细节</strong> 模型训练有一些固定的参数，作者使用batch size为512、动量为0.9、weight decay为1e-4的SGD来训练所有模型。作者训练90个阶段，初始学习率为0.1，每30个阶段下降10倍。对于标准训练，作者采用标准交叉熵多类分类损失。对于健壮性训练，作者使用对抗性训练[MMS+18]。作者训练对抗样本产生的最大允许扰动 $$l_{2}$$ of  ε∈ {0.01、0.03、0.05、0.1、0.25、0.5、1、3、5}以及最大允许扰动 $$l_{∞}$$ of  ε∈ {$$\frac{0.5}{255},\frac{1}{255},\frac{2}{255},\frac{4}{255},\frac{8}{255}$$}使用3个攻击步骤，每个攻击步骤步长为$$\frac{2 ε}{3}$$。</p><p>在图 2 中，我们将标准模型的下游传输精度与具有相同架构的最佳稳健模型的下游传输精度进行了比较（在 ε上进行网格搜索）。 结果表明，与标准网络相比，稳健网络始终为迁移学习提取更好的特征——这种效果在 Aircraft、CIFAR-10、CIFAR-100、Food、SUN397 和 Caltech-101 上最为明显。 由于计算限制，我们无法在相同数量的鲁棒性级别 ε 下训练 WideResNet-50-4x 模型，因此使用了较粗的网格。 因此，对 ε 进行更精细的网格搜索可能会进一步改善结果。</p><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210609204723553.png" alt="image-20210609204723553" style="zoom:50%;"></li><li><h5 id="What-datasets-are-built-used-for-the-quantitative-evaluation-Is-the-code-open-sourced-客观描述：用于定量评估的数据集是什么？代码有没有开源？"><a href="#What-datasets-are-built-used-for-the-quantitative-evaluation-Is-the-code-open-sourced-客观描述：用于定量评估的数据集是什么？代码有没有开源？" class="headerlink" title="What datasets are built/used for the quantitative evaluation? Is the code open sourced? 客观描述：用于定量评估的数据集是什么？代码有没有开源？"></a>What datasets are built/used for the quantitative evaluation? Is the code open sourced? 客观描述：用于定量评估的数据集是什么？代码有没有开源？</h5><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210531233156838.png" alt="image-20210531233156838" style="zoom:30%;"><p>作者在[KSL19]中使用的分类数据集上测试从ImageNet预训练模型开始的迁移学习。这些数据集的大小因类和数据点的数量而异。</p><p>对于这种类型的迁移学习，冻结 ImageNet 预训练模型的权重，并用一个随机初始化的适合迁移数据集的全连接层替换最后一个全连接层。 仅使用 SGD 训练这个新层 150 个 epoch，批大小为 64，动量为 0.9，权重衰减为 5e - 4，初始 LR ∈ {0.01, 0.001} 每 50 个 epoch 下降 10 倍 。</p><p><strong>代码已开源</strong>： <a href="https://github.com/Microsoft/robust-models-transfer" target="_blank" rel="noopener">https://github.com/Microsoft/robust-models-transfer</a></p></li><li><h5 id="Is-the-scientific-hypothesis-well-supported-by-evidence-in-the-experiments-论文中的实验及结果有没有很好地支持需要验证的科学假设？"><a href="#Is-the-scientific-hypothesis-well-supported-by-evidence-in-the-experiments-论文中的实验及结果有没有很好地支持需要验证的科学假设？" class="headerlink" title="Is the scientific hypothesis well supported by evidence in the experiments?论文中的实验及结果有没有很好地支持需要验证的科学假设？"></a>Is the scientific hypothesis well supported by evidence in the experiments?论文中的实验及结果有没有很好地支持需要验证的科学假设？</h5><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210531234340123.png" alt="image-20210531234340123" style="zoom:30%;"><p>为了验证这一假设，作者首先研究了他训练的每个稳健模型的ImageNet精度和传输精度之间的关系。作者发现，一旦稳健性方面发挥作用，先前观察到的准确度和传输性能之间的线性关系往往会被破坏。更直接地支持假设，即稳健性和ImageNet准确度对传输有不同的影响，作者发现，当稳健性水平保持不变时，标准模型的精度传递相关性实际上也适用于稳健性模型ε=3。因此，六种不同结构的模型精度较低，并将ImageNet的精度与迁移学习性能进行了比较。对于这些模型，提高ImageNet的准确度可以提高模型迁移性能，提高的准确率与标准模型相当（并且比标准模型具有更高的相关性）。这些观察结果表明，可以通过应用提高鲁棒模型准确度的已知技术来进一步提高迁移学习性能（如[BGH19；CRS+19]）。除此之外，作者的研究结果还表明准确度并不能充分衡量特征质量，理解为什么健壮的网络传输特别好仍然是一个悬而未决的问题，可能与以前分析这些网络使用的特性的工作有关。</p><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210531234318900.png" alt="image-20210531234318900" style="zoom:30%;"><p>作者的实验还揭示了鲁棒模型和标准模型在传递性能随模型宽度变化方面的对比。虽然增加网络深度可以改善传输性能，但增加网络宽度会损害传输性能。随着宽度的增加，标准模型的传输性能平台和下降，但对于稳健的模型来说，仍在稳步增长。这表明与标准网络相比，扩展网络宽度可以进一步提高鲁棒网络的迁移性能。</p><p>接着，作者将这一研究结果延续到这全网络迁移学习的三个应用：图像分类、目标检测和实例分割进行了实验。</p><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210601005000835.png" alt="image-20210601005000835" style="zoom:30%;"><p>对于12个下游图像分类任务，使用标准和稳健模型的全网络转移学习结果。可以发现鲁棒模型在迁移学习性能方面与标准模型匹配或有提升。</p><img src="/Users/mac/Library/Application Support/typora-user-images/image-20210601005421962.png" alt="image-20210601005421962" style="zoom:33%;"><p>对于目标标检测和实例分割同样可以发现鲁棒的模型的迁移学习性能更强。</p></li><li><h5 id="What-are-the-contributions-of-the-paper-What-could-be-done-next"><a href="#What-are-the-contributions-of-the-paper-What-could-be-done-next" class="headerlink" title="What are the contributions of the paper? What could be done next?"></a>What are the contributions of the paper? What could be done next?</h5><p>在这项工作中使用对抗鲁棒模型的迁移学习，比较了鲁棒模型和标准模型在12个分类任务、目标检测和实例分割上的学习性能。实验结果表明，尽管ImageNet的精确度较低，但对抗性鲁棒神经网络的性能始终与标准神经网络的性能相匹配或提高。这些观察结果表明，可以通过应用提高鲁棒模型准确度的已知技术来进一步提高迁移学习性能（如[BGH19；CRS+19]）。</p><p>文章还进一步研究了对抗鲁棒网络的行为，并研究了ImageNet精度、模型宽度、鲁棒性和迁移性能之间的相互作用。虽然增加网络深度可以改善传输性能，但增加网络宽度会损害传输性能。随着宽度的增加，标准模型的传输性能平台和下降，但对于稳健的模型来说，仍在稳步增长。这表明与标准网络相比，扩展网络宽度可以进一步提高鲁棒网络的迁移性能。本文偏重实验着重从实验的角度出发告诉了我们这样一个理解为什么健壮的网络传输特别好仍然是一个悬而未决的问题。下一步研究可以着重从理论的角度去解释为什么健壮的网络迁移效果比较好。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>专业英语</title>
      <link href="/2021/05/29/zhuan-ye-ying-yu/"/>
      <url>/2021/05/29/zhuan-ye-ying-yu/</url>
      
        <content type="html"><![CDATA[<p>在实验或看论文中遇到的一些英语学习，学习！</p><a id="more"></a><h5 id="Gradient-overflow-梯度溢出"><a href="#Gradient-overflow-梯度溢出" class="headerlink" title="Gradient overflow 梯度溢出"></a>Gradient overflow 梯度溢出</h5><p>Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to 131072.0</p><p>解决办法：</p><p>经过验证可以通过以下几种方法，来防止出现梯度溢出的问题：</p><p>1、O2换成O1，再不行换成O0</p><p>2、把batchsize从32调整为16会显著解决这个问题，另外在换成O0的时候会出现内存不足的情况，减小batchsize也是有帮助的</p><p>3、减少学习率也是一种方法</p><p>4、增加Relu会有效保存梯度，防止梯度消失</p><p>opt_level</p><p>其中只有一个opt_level需要用户自行配置：</p><p>O0：纯FP32训练，可以作为accuracy的baseline；</p><p>O1：混合精度训练（推荐使用），根据黑白名单自动决定使用FP16（GEMM, 卷积）还是FP32（Softmax）进行计算。</p><p>O2：“几乎FP16”混合精度训练，不存在黑白名单，除了Batch norm，几乎都是用FP16计算。之前是这个</p><p>O3：纯FP16训练，很不稳定，但是可以作为speed的baseline；</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Gpu queue</title>
      <link href="/2021/05/09/gpu-queue/"/>
      <url>/2021/05/09/gpu-queue/</url>
      
        <content type="html"><![CDATA[<p>Bbkbkbjk</p><a id="more"></a><h5 id="GPU排队脚本——一旦GPU空闲，自动触发python程序"><a href="#GPU排队脚本——一旦GPU空闲，自动触发python程序" class="headerlink" title="GPU排队脚本——一旦GPU空闲，自动触发python程序"></a>GPU排队脚本——一旦GPU空闲，自动触发python程序</h5><p>今天写了一个GPU排队脚本，事实上还是挺实用的。有的服务器是多用户使用，GPU的资源常常被占据着，很可能在夜间GPU空闲了，但来不及运行自己的脚本。如果没有和别人共享服务器的话，自己的多个程序想排队使用GPU，也可以用这个脚本。环境非常简单，有Python就行了：</p><p>先创建脚本：</p><pre class="line-numbers language-python"><code class="language-python">vim gputask<span class="token punctuation">.</span>py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 style="font-family:-----,normal;font-size:-----px;color:-----" <h2>  </h2><p>监控有空闲资源的GPU并发送邮件</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> email<span class="token punctuation">.</span>mime<span class="token punctuation">.</span>text <span class="token keyword">import</span> MIMEText<span class="token keyword">from</span> email<span class="token punctuation">.</span>header <span class="token keyword">import</span> Header<span class="token keyword">from</span> smtplib <span class="token keyword">import</span> SMTP_SSL<span class="token keyword">from</span> pynvml <span class="token keyword">import</span> <span class="token operator">*</span><span class="token keyword">import</span> schedule<span class="token keyword">import</span> time<span class="token keyword">import</span> datetime<span class="token keyword">def</span> <span class="token function">mail_fun</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true">#qq邮箱smtp服务器</span>    host_server <span class="token operator">=</span> <span class="token string">'smtp.qq.com'</span>    <span class="token comment" spellcheck="true">#sender_qq为发件人的qq号码</span>    sender_qq <span class="token operator">=</span> <span class="token string">'1*****5@qq.com'</span>    <span class="token comment" spellcheck="true">#pwd为qq邮箱的授权码</span>    pwd <span class="token operator">=</span> <span class="token string">'a*****jb'</span> <span class="token comment" spellcheck="true">## xh**********bdc</span>    <span class="token comment" spellcheck="true">#发件人的邮箱</span>    sender_qq_mail <span class="token operator">=</span> <span class="token string">'1****5@qq.com'</span>    <span class="token comment" spellcheck="true">#收件人邮箱</span>    receiver <span class="token operator">=</span> <span class="token string">'1*******5@qq.com'</span> <span class="token comment" spellcheck="true">#邮件的正文内容</span>    mail_content <span class="token operator">=</span> <span class="token string">'gpu 有卡'</span>    <span class="token comment" spellcheck="true">#邮件标题</span>    mail_title <span class="token operator">=</span> <span class="token string">'gpu的邮件'</span>    <span class="token comment" spellcheck="true">#ssl登录</span>    smtp <span class="token operator">=</span> SMTP_SSL<span class="token punctuation">(</span>host_server<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#set_debuglevel()是用来调试的。参数值为1表示开启调试模式，参数值为0关闭调试>模式</span>    smtp<span class="token punctuation">.</span>set_debuglevel<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    smtp<span class="token punctuation">.</span>ehlo<span class="token punctuation">(</span>host_server<span class="token punctuation">)</span>    smtp<span class="token punctuation">.</span>login<span class="token punctuation">(</span>sender_qq<span class="token punctuation">,</span> pwd<span class="token punctuation">)</span>    msg <span class="token operator">=</span> MIMEText<span class="token punctuation">(</span>mail_content<span class="token punctuation">,</span> <span class="token string">"plain"</span><span class="token punctuation">,</span> <span class="token string">'utf-8'</span><span class="token punctuation">)</span>    msg<span class="token punctuation">[</span><span class="token string">"Subject"</span><span class="token punctuation">]</span> <span class="token operator">=</span> Header<span class="token punctuation">(</span>mail_title<span class="token punctuation">,</span> <span class="token string">'utf-8'</span><span class="token punctuation">)</span>    msg<span class="token punctuation">[</span><span class="token string">"From"</span><span class="token punctuation">]</span> <span class="token operator">=</span> sender_qq_mail    msg<span class="token punctuation">[</span><span class="token string">"To"</span><span class="token punctuation">]</span> <span class="token operator">=</span> receiver    smtp<span class="token punctuation">.</span>sendmail<span class="token punctuation">(</span>sender_qq_mail<span class="token punctuation">,</span> receiver<span class="token punctuation">,</span> msg<span class="token punctuation">.</span>as_string<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    smtp<span class="token punctuation">.</span>quit<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">check_nvi</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    nvmlInit<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true">#初始化</span>    <span class="token comment" spellcheck="true">#查看设备</span>    deviceCount <span class="token operator">=</span> nvmlDeviceGetCount<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>deviceCount<span class="token punctuation">)</span><span class="token punctuation">:</span>        handle <span class="token operator">=</span> nvmlDeviceGetHandleByIndex<span class="token punctuation">(</span>i<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># print("GPU", i, ":", nvmlDeviceGetName(handle))</span>        <span class="token comment" spellcheck="true"># 查看当前显卡 剩余显存大小pip</span>        free_info <span class="token operator">=</span> nvmlDeviceGetMemoryInfo<span class="token punctuation">(</span>handle<span class="token punctuation">)</span><span class="token punctuation">.</span>free <span class="token operator">/</span> <span class="token number">1024</span> <span class="token operator">/</span> <span class="token number">1024</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"GPU"</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token string">":"</span><span class="token punctuation">,</span> str<span class="token punctuation">(</span>free_info<span class="token punctuation">)</span><span class="token operator">+</span><span class="token string">"M"</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 剩余显存 大于9000 邮件通知</span>        <span class="token keyword">if</span> free_info <span class="token operator">>=</span> <span class="token number">5000</span><span class="token punctuation">:</span>          <span class="token comment" spellcheck="true"># 判断当前时间是否是在 工作时间（工作时间才通知 8-24）</span>            <span class="token comment" spellcheck="true"># 当前时间（第几个小时）</span>            curr_hour <span class="token operator">=</span> time<span class="token punctuation">.</span>localtime<span class="token punctuation">(</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">.</span>tm_hour            <span class="token keyword">if</span> curr_hour <span class="token operator">>=</span> <span class="token number">0</span> <span class="token operator">and</span> curr_hour <span class="token operator">&lt;=</span><span class="token number">24</span><span class="token punctuation">:</span>                mail_fun<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">break</span>    <span class="token comment" spellcheck="true">#最后要关闭管理工具</span>    nvmlShutdown<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">job</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    check_nvi<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 定时任务</span>schedule<span class="token punctuation">.</span>every<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>minutes<span class="token punctuation">.</span>do<span class="token punctuation">(</span>job<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 每30分钟执行一次</span><span class="token comment" spellcheck="true">#schedule.every(30).seconds.do(job) # 每10秒执行一次</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># mail_fun()</span>    <span class="token comment" spellcheck="true"># check_nvi()</span>    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>        schedule<span class="token punctuation">.</span>run_pending<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 运行所有可运行的任务</span>        time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>5W2H | 关于写博客的七点反思</title>
      <link href="/2020/01/14/5w2h-godweiyang/"/>
      <url>/2020/01/14/5w2h-godweiyang/</url>
      
        <content type="html"><![CDATA[<p>Bbkbkbjk</p><a id="more"></a><blockquote><p>关注公众号【算法码上来】，每日算法干货马上就来！</p></blockquote><p><img src="/medias/contact.jpg" alt></p><h2 id="When？什么时候开始写的？"><a href="#When？什么时候开始写的？" class="headerlink" title="When？什么时候开始写的？"></a>When？什么时候开始写的？</h2><p>第一次写博客是 2015 年了，在 CSDN 和博客园上面，当时写文章是为了记录 ACM 竞赛的题解，陆陆续续写了几十篇。但是最后还是没有坚持下去，主要还是因为 CSDN 和博客园的体验太差了，写起文章来很难受，又丑又慢。另一个原因是， CSDN 发个文章还需要审核，定制主题自由度也太差。</p><p>第二次就是 2017 年暑假了，当时自己建了个人博客，域名是：<a href="https://godweiyang.com/" target="_blank" rel="noopener">godweiyang.com</a>。当时的想法是，个人博客好好看，有各种主题，如果会点前端知识，还能自己魔改。个人博客主要更新的就是自然语言处理的知识了，主要都是些论文的阅读赏析。另外还更新一些计算机相关的基础知识，主要考虑到绝大多数人并不会对枯燥又专业的自然语言处理感兴趣。算法题解的话最近也开始更新起来了，主要写一些 LeetCode上面的题解。</p><p>最后就是知乎专栏和微信公众号了，这两个平台是最近才搞的，主要把个人博客的文章同步更新过去，内容都差不多。微信公众号本来不想搞的，不支持 markdown ，写起来挺麻烦的。但是考虑到以后的发展，以及可能会有一些变现的操作，还是重操旧业了（其实我公众号两年前就注册过了，只是一直没有更新）。</p><h2 id="Why？为什么会想起来写博客？"><a href="#Why？为什么会想起来写博客？" class="headerlink" title="Why？为什么会想起来写博客？"></a>Why？为什么会想起来写博客？</h2><p>其实刚开始写博客，主要还是为了记录自己平时学到的一些东西，以后可能还能翻出来复习复习。但是现在看来，基本不会再去翻以前写的东西了。</p><p>另一个目的，也是我写博客最主要的目的，还是想分享我知道的一些东西，能够让更多的人因此受益。因为写博客，其实还是认识了不少天南地北的朋友的，有各个高校甚至初高中的学生，也有工作了很多年想学习学习编程的，也有国外一些名校的学生。不管是谁，我觉得都可以扩展我的人脉，现在微信好友人数上限扩展到了 5000 人了，而我才用了十分之一多一点，什么时候能达到上限也算是圆满了。</p><p>最后，有句话叫做：“不以盈利为目的的博客最后都死亡了”。因为如果不能因此获得任何的收入的话，极少有人有这个毅力坚持更新博客。虽然我也想因此获利，但是暂时我并不想因此改变了初衷，去写一些刻意迎合大众的水文，从而获得粉丝。</p><h2 id="What？都写一些什么主题的博客？"><a href="#What？都写一些什么主题的博客？" class="headerlink" title="What？都写一些什么主题的博客？"></a>What？都写一些什么主题的博客？</h2><p>主要都是与我相关的一些计算机相关知识。最主要的就是深度学习和自然语言处理了，但是因为我是做句法分析的，这个方向受众比较小，如果纯粹写这个方向的内容的话，看的人可能会很少。而如果写深度学习和自然语言处理的入门普及或者综合一点热门一点的方向的话，看得人应该会很多。但是这样又有一个弊端，非常的浪费我时间，因为过于基础的知识对我的提升微乎其微，只适合以后我面试前看看补补基础。而主要我现在重心还在发论文，继续搞学术，所以只能写一些结合我最近所学知识的文章。</p><p>还会写一些算法题解，现在主要在做 LeetCode 上面的中等和困难题目，然后每日一更。尽管网上各种 LeetCode 的题解已经太多了，但是我觉得，大多数的题解都非常的模糊，讲解的很不清楚，抄来抄去的有什么意思？而他们的代码风格，更是让我看不下去，最基本的美观都做不到，代码的简洁精炼就更不用谈了。所以我的目的还是尽量用美观精炼的代码来让更多人的算法思想和代码能力得到提升。</p><p>偶尔，我还会分享一些计算机基础知识，比如怎么搭博客（这是我阅读量最高的一篇博客了）、好用的软件、常用的一些软件的安装配置等等。其实讲道理，我是不愿意写这一类文章的，因为非常的费时费力，需要自己动手模拟一遍，确保能够正确完成才能写进文章。不然就会像很多网上的教程那样，抄来抄去一堆错误，根本没有自己动手实践过。但是这种文章其实对很多人帮助还挺大的，大家也乐意去看，如果你在知乎发的话，你会发现这类文章收藏量都挺高的。如果我以后用空了，我还是会经常把我平时的一些经验分享给大家的，比如 LaTeX 常用写作技巧之类的。</p><p>那么其他火的博客都写些什么呢？我觉得当代人闲暇时候、上厕所刷手机的时候，那些碎片时间是没办法好好学习一些知识的，最爱看的还是有趣的故事，例如 99 行代码做出冰雪奇缘特效、程序员脱发、程序员单身狗啦之类的。这些故事背后的技术可能是很深奥的，但是大多数人并不会去关心，只是会看完惊叹一句“卧槽”而已。其他的吸引人的内容还有资料下载、课程学习（当然基本都是广告）等等。不过我个人目前并不想写这些东西，首先这些新闻类的文章很多地方都有了，写来写去就那么些东西，同质化严重。而资料下载确实是个不错的主意，可以分享好东西给大家，但是我个人最近也没有太多值得珍藏的好资料，况且大多数网上都有现成的，我不赞同为了增加粉丝而不放出链接，把资料放在公众号后台的行为。</p><p>总之，不忘初心，粉丝什么的随缘吧。我非常佛系，即使没人看我也会日常更新，就当记日记不是吗？</p><h2 id="Who？写出来的博客都是给谁看的？"><a href="#Who？写出来的博客都是给谁看的？" class="headerlink" title="Who？写出来的博客都是给谁看的？"></a>Who？写出来的博客都是给谁看的？</h2><p>大多数看我博客的都是自然语言处理相关的学生。就拿我在知乎专栏的粉丝来看，大多数人是来看我的论文赏析的，而少部分人是看到了那篇博客搭建教程来的。至于算法题解，貌似并没有很多人看，可能这一类文章网上实在是太多了，没有什么独特的吸引力。而公众号粉丝，目前为止还很少，绝大多数还都是好友粉丝。主要原因还是公众号太封闭了，很难让好友之外的人知道并且关注你。一个方法可以在知乎等平台引流，但是现在这属于违规操作，最好还是不要干了。</p><p>其实我目标的受众群体还是包括但不仅限于计算机系的学生，像一些计算机基础知识，就算你是个小白，也可以从这里学到很多东西。甚至还有很多考研的学生来咨询我问题，虽然我是保研的，但是很多导师选取、学习方面的问题我还是愿意给出我自己的建议的。我这个人向来喜欢分享，也喜欢倾听，如果别人来问我问题，我基本是会耐心回答的。曾经有啥都不会的小白来问我怎么搭建博客，我得从最基本的命令行教他，讲道理这其实很烦，我都不想教，但我还是会尽量把关键点都告诉他。有人会觉得，我这样最后会得到什么呢？是的，也不赚钱，最多偶尔有些朋友会打赏个红包，我还浪费了大把时间（其实还好，白天工作的时候我都简单回复，晚上一般会详细点），但是认识的人多了，许多人还是多少知道我这个名字的，虽然没啥用，但是也算是种隐形财富吧，以后有什么用再说。</p><h2 id="Where？都在什么平台写博客呢？"><a href="#Where？都在什么平台写博客呢？" class="headerlink" title="Where？都在什么平台写博客呢？"></a>Where？都在什么平台写博客呢？</h2><p>这个其实上面都说过了，我现在文章主要更新在个人博客、知乎专栏、微信公众号和 CSDN 上面，我简要说一些这些平台的优缺点吧。</p><p>个人博客可以个性化定制主题，想怎么好看就怎么好看，还不用发文章审核，想发什么就发什么。但是缺点就是发文章麻烦一点的，步骤略多，并且搜索引擎收录很慢的。</p><p>知乎专栏是我比较喜欢的一个平台，主要是它支持markdown，还可以把公式变成矢量图片，还是挺不错的。但是发知乎文章要注意千万别带着营销、引流等内容，不然容易被人举报被删除甚至禁言。</p><p>微信公众号是我最近才开始运营的，相对而言，它的编辑界面是最不友好的一个，只支持富文本编辑，连markdown都没有。但是也有解决方法，比如我现在用<a href="https://mdnice.com/" target="_blank" rel="noopener">mdnice.com</a>这个网站，把markdown转换成微信公众号的富文本格式，还是非常好用的。公众号还有个缺点，太封闭了，很难宣传出去，刚开始只能亲朋好友关注关注，但是如果你真的做大了知名度，还是可以借助微信的优势，赚得不少广告钱的。</p><p>CSDN因为几年前被永久封过号，然后现在不知道为什么又被解封了，所以用的不是特别多，主要都是通过知乎专栏自动同步文章过去的。CSDN 好处就是流量大，很多人都在上面搜东西，百度搜出来也基本都是 CSDN ，但是排版是真的真的烂，不过现在好多了，也支持 markdown 了。但是还是谈不上喜欢，广告什么的都太多了，除非迫不得已，我一般不去 CSDN 看技术文章，我一般都是个人博客或者知乎看论文解读之类的。</p><h2 id="How？按照什么流程来写博客？"><a href="#How？按照什么流程来写博客？" class="headerlink" title="How？按照什么流程来写博客？"></a>How？按照什么流程来写博客？</h2><p>其实同时维护好几个平台还是挺累的，你得找到一个最佳的顺序来发布文章，这样才能事半功倍。我一般都是先在个人博客上写好 markdown 文章，然后发布完之后，打开<a href="https://mdnice.com/" target="_blank" rel="noopener">mdnice.com</a>在线编辑网站，把 markdown 粘贴进去，转成微信公众号和知乎专栏的格式，最后分别发布在两个地方就行了。 CSDN 就不用管了，它会每天自动同步知乎专栏的文章的，倒是为我省了不少事。markdown 写作也挺轻松的，完全不用管排版之类的问题，安安心心写内容就行了。</p><p>微信公众号现在设置的是每天早上 8:05 推送，其实稍微晚一点比较好，这样别人的都推送完了，你的就会置顶在最上面。而知乎专栏和个人博客我就随性发布了，想什么时候发就什么时候发，经常会前一天就写好内容，早早的发布出去了。</p><p>我个人现在来看，写一篇博客最累的是敲公式和找图片，当然像我这篇就一个公式和图都没有，是最最轻松的了。我这个人有强迫症，公式一定要手打 LaTeX 公式，然后转成矢量图才行，这样看着又清楚又舒服。遇到复杂的矩阵公式，能把我敲的头晕。这也是我为什么不喜欢 CSDN 的一个原因，上面很多文章公式全是截图，看的我头都大了。</p><h2 id="How-much？要花多少时间和金钱来运营维护？"><a href="#How-much？要花多少时间和金钱来运营维护？" class="headerlink" title="How much？要花多少时间和金钱来运营维护？"></a>How much？要花多少时间和金钱来运营维护？</h2><p>写博客挺花时间的，特别是现在还在为发论文而忙活的时候，可能论文截稿前那段时间甚至我会停更好久哈哈。不过现在放假了，还算比较闲，有功夫搞搞这些。我个人是倾向于白天还是老老实实学习吧，晚上把部分的游戏时间抽出来写会儿文章，更新一波。这样也不算太浪费时间，毕竟就算不写，时间也都用来打游戏了，游戏输了还坏了心情（我的亚索怎么会输？）。</p><p>那什么时候做 LeetCode 呢？我一般都是白天吃饭时，或者走在路上时，手机 app 上随机选一道题，然后吃个饭的时间就能想出个解法，回到实验室后敲一顿代码通过了就行了。这样看来也节约了不少时间嘛，还能和室友一起讨论讨论，帮他提高一波算法能力。</p><p>金钱的话就基本没有花销了，除了开了个素材设计网站的会员（我是真的睿智，用 PS 不就行了嘛，脑子瓦特了充钱了）以外，其他不需要啥了，偶尔还能吃点打赏钱，粉丝太少了，没有广告。</p><p>因为写文章这上面花时间比较多，写出好的文章、不水文章的话花的时间更多，所以很容易坚持不下去。而又要迎合大众的口味，众口难调嘛，又要坚持自己的初心，尽量写对自己提升大的内容，还是比较难以平衡的。特别是粉丝特别少，没有什么人看的时候，你会很想放弃。</p><p>但是，总会有人在你坚持不下去的时候，给你鼓励的目光，支持你继续走下去的。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
